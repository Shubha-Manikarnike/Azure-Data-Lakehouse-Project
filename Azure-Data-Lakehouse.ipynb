{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "516e8674-c557-4c57-8643-ae0ae6808839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, sequence, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "541e22ca-c0dc-44aa-b158-0d03f5c1a55a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[36]: [(&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;1&#39;),\n",
       " (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/databricks-hive/*&#39;),\n",
       " (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n",
       " (&#39;spark.sql.warehouse.dir&#39;, &#39;dbfs:/user/hive/warehouse&#39;),\n",
       " (&#39;spark.databricks.managedCatalog.clientClassName&#39;,\n",
       "  &#39;com.databricks.managedcatalog.ManagedCatalogClientImpl&#39;),\n",
       " (&#39;spark.app.id&#39;, &#39;app-20220902094120-0000&#39;),\n",
       " (&#39;spark.hadoop.fs.gs.impl&#39;,\n",
       "  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem&#39;),\n",
       " (&#39;spark.executor.extraJavaOptions&#39;,\n",
       "  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n",
       " (&#39;spark.executor.memory&#39;, &#39;7284m&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;1&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-s3.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.retry.limit&#39;, &#39;20&#39;),\n",
       " (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n",
       "  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterLastActivityTime&#39;,\n",
       "  &#39;1661829498609&#39;),\n",
       " (&#39;spark.databricks.service.dbutils.repl.backend&#39;,\n",
       "  &#39;com.databricks.dbconnect.ReplDBUtils&#39;),\n",
       " (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;10.4.x-scala2.12&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n",
       " (&#39;spark.databricks.acl.dfAclsEnabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.effectiveSparkVersion&#39;,\n",
       "  &#39;10.4.x-scala2.12&#39;),\n",
       " (&#39;spark.hadoop.fs.cpfs-adl.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException&#39;,\n",
       "  &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.fs.mcfs-s3.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n",
       " (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.driver.extraJavaOptions&#39;,\n",
       "  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.isIMv2Enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.hive.hmshandler.retry.interval&#39;, &#39;2000&#39;),\n",
       " (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n",
       " (&#39;spark.hadoop.fs.azure.authorization.caching.enable&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-abfss.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n",
       " (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n",
       "  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n",
       " (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;Standard_DS3_v2&#39;),\n",
       " (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.userId&#39;, &#39;1708273445324017&#39;),\n",
       " (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n",
       " (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterName&#39;,\n",
       "  &#34;student_10f0d7tol6kadyw7&#39;s Cluster&#34;),\n",
       " (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;, &#39;10.139.0.5&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;1&#39;),\n",
       " (&#39;spark.databricks.sql.configMapperClass&#39;,\n",
       "  &#39;com.databricks.dbsql.config.SqlConfigMapperBridge&#39;),\n",
       " (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-s3.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n",
       " (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.sql.legacy.createHiveTableByDefault&#39;, &#39;false&#39;),\n",
       " (&#39;spark.ui.port&#39;, &#39;40001&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-s3a.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.attempts.maximum&#39;, &#39;10&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough&#39;,\n",
       "  &#39;false&#39;),\n",
       " (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;Standard_DS3_v2&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-s3n.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.retry.throttle.interval&#39;, &#39;500ms&#39;),\n",
       " (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n",
       " (&#39;spark.databricks.wsfsPublicPreview&#39;, &#39;true&#39;),\n",
       " (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes&#39;,\n",
       "  &#39;false&#39;),\n",
       " (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n",
       "  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n",
       " (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.s3.impl&#39;,\n",
       "  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n",
       " (&#39;spark.hadoop.hive.hmshandler.retry.attempts&#39;, &#39;10&#39;),\n",
       " (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmpQQVKWf&#39;),\n",
       " (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n",
       " (&#39;spark.sql.sources.default&#39;, &#39;delta&#39;),\n",
       " (&#39;spark.hadoop.fs.mcfs-gs.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.cpfs-s3n.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.cpfs-adl.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-s3n.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.cpfs-abfss.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n",
       " (&#39;spark.databricks.passthrough.oauth.refresher.impl&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient&#39;),\n",
       " (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n",
       "  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n",
       " (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n",
       " (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n",
       " (&#39;spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider&#39;),\n",
       " (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterMinWorkers&#39;, &#39;1&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.mcfs-s3a.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n",
       " (&#39;spark.app.startTime&#39;, &#39;1662111677593&#39;),\n",
       " (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v2&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;1708273445324017&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n",
       " (&#39;spark.driver.host&#39;, &#39;10.139.64.5&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n",
       "  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;southindia&#39;),\n",
       " (&#39;spark.hadoop.fs.elfs.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.credential.redactor&#39;,\n",
       "  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.acl.provider&#39;,\n",
       "  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n",
       " (&#39;spark.hadoop.parquet.abfs.readahead.optimization.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.extraListeners&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n",
       " (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2&#39;, &#39;0&#39;),\n",
       " (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;Standard_DS3_v2&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterMaxWorkers&#39;, &#39;2&#39;),\n",
       " (&#39;spark.hadoop.fs.dbfs.impl&#39;,\n",
       "  &#39;com.databricks.backend.daemon.data.client.DBFS&#39;),\n",
       " (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.cpfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.tahoe.logStore.class&#39;,\n",
       "  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n",
       " (&#39;libraryDownload.sleepIntervalSeconds&#39;, &#39;5&#39;),\n",
       " (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.service.dbutils.server.backend&#39;,\n",
       "  &#39;com.databricks.dbconnect.SparkServerDBUtils&#39;),\n",
       " (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n",
       " (&#39;spark.databricks.managedCatalog.s3a.tokenProviderClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider&#39;),\n",
       " (&#39;spark.databricks.repl.enableClassFileCleanup&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;, &#39;10.139.64.5&#39;),\n",
       " (&#39;spark.driver.port&#39;, &#39;36515&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n",
       " (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n",
       " (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled&#39;,\n",
       "  &#39;true&#39;),\n",
       " (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;[REDACTED]&#39;),\n",
       " (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n",
       "  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting&#39;,\n",
       "  &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n",
       "  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n",
       " (&#39;spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.managedResourceGroup&#39;,\n",
       "  &#39;databricks-rg-L4demo-oumcmsuqcy3hu&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n",
       " (&#39;libraryDownload.timeoutSeconds&#39;, &#39;180&#39;),\n",
       " (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.azureSubscriptionId&#39;,\n",
       "  &#39;94ec3a64-dcfe-4219-9e29-88221690c382&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n",
       "  &#39;workerenv-8172574838468005&#39;),\n",
       " (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.block.size&#39;, &#39;67108864&#39;),\n",
       " (&#39;spark.databricks.tahoe.logStore.gcp.class&#39;,\n",
       "  &#39;com.databricks.tahoe.store.GCPLogStore&#39;),\n",
       " (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkMasterUrlType&#39;, &#39;None&#39;),\n",
       " (&#39;spark.databricks.passthrough.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.sql.sources.commitProtocolClass&#39;,\n",
       "  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n",
       " (&#39;spark.hadoop.fs.abfss.impl&#39;,\n",
       "  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-s3a.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.attribute_tag_budget&#39;, &#39;&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n",
       "  &#39;azure_disk_volume_type: PREMIUM_LRS\\n&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0830-030640-xkxwpaff&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n",
       " (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.file.impl&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.mcfs-s3n.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-wasb.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.cpfs-s3.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer&#39;, &#39;&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n",
       " (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n",
       " (&#39;spark.hadoop.fs.elfs.impl&#39;,\n",
       "  &#39;com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND_AZURE&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_suite&#39;, &#39;&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-wasbs.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;1&#39;),\n",
       " (&#39;spark.databricks.driverNfs.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n",
       "  &#39;RDS_DIRECT&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.ngrokNpipEnabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.parquet.page.metadata.validation.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.acl.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.passthrough.glue.executorServiceFactoryClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory&#39;),\n",
       " (&#39;spark.repl.class.outputDir&#39;,\n",
       "  &#39;/local_disk0/tmp/repl/spark-4647372778270552063-8510db57-0394-4867-84a2-bc1aa57f65d6&#39;),\n",
       " (&#39;spark.hadoop.fs.abfs.impl&#39;,\n",
       "  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.acl.scim.client&#39;,\n",
       "  &#39;com.databricks.spark.sql.acl.client.DriverToWebappScimClient&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;true&#39;),\n",
       " (&#39;spark.executor.extraClassPath&#39;,\n",
       "  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----com_google_protobuf--timestamp_proto-spark_3.2_2.12-scalabp.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_2--common--kvstore--kvstore-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-common--network-common-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-shuffle--network-shuffle-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--sketch--sketch-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--tags--tags-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--unsafe--unsafe-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_2--core--proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--launcher--launcher-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.5.0-4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.gson--gson--com.google.code.gson__gson__2.8.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.istack--istack-commons-runtime--com.sun.istack__istack-commons-runtime__3.0.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill-java--com.twitter__chill-java__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill_2.12--com.twitter__chill_2.12__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.zaxxer--HikariCP--com.zaxxer__HikariCP__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-codec--commons-codec--commons-codec__commons-codec__1.15.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-io--commons-io--commons-io__commons-io__2.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--arpack--dev.ludovic.netlib__arpack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--blas--dev.ludovic.netlib__blas__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--lapack--dev.ludovic.netlib__lapack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.airlift--aircompressor--io.airlift__aircompressor__0.21.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.4.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.netty--netty-all--io.netty__netty-all__4.1.68.Final.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.servlet--jakarta.servlet-api--jakarta.servlet__jakarta.servlet-api__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.3.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--joda-time--joda-time--joda-time__joda-time__2.10.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.avro--avro--org.apache.avro__avro__1.10.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.10.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.avro--avro-mapred--org.apache.avro__avro-mapred__1.10.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.21.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-text--org.apache.commons__commons-text__1.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.curator--curator-client--org.apache.curator__curator-client__2.13.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.13.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.13.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.derby--derby--org.apache.derby__derby__10.14.2.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hadoop--hadoop-client-api--org.apache.hadoop__hadoop-client-api__3.3.1-databricks.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hadoop--hadoop-client-runtime--org.apache.hadoop__hadoop-client-runtime__3.3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.9.jar:/databricks/jars/----workspace_sp\n",
       "*** WARNING: skipped 82959 bytes of output ***\n",
       "\n",
       " (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.assumed.role.credentials.provider&#39;,\n",
       "  &#39;com.amazonaws.auth.InstanceProfileCredentialsProvider&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.fast.upload.active.blocks&#39;, &#39;32&#39;),\n",
       " (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n",
       "  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-abfs.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.orgId&#39;, &#39;8172574838468005&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.repl.class.uri&#39;, &#39;spark://10.139.64.5:36515/classes&#39;),\n",
       " (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n",
       " (&#39;spark.databricks.cloudProvider&#39;, &#39;Azure&#39;),\n",
       " (&#39;spark.databricks.cloudfetch.hasRegionSupport&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories&#39;,\n",
       "  &#39;false&#39;),\n",
       " (&#39;spark.hadoop.fs.wasb.impl&#39;,\n",
       "  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.mcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.unityCatalog.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.passthrough.glue.credentialsProviderFactoryClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory&#39;),\n",
       " (&#39;spark.sparklyr-backend.threads&#39;, &#39;1&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice&#39;, &#39;-1.0&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-wasb.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n",
       " (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n",
       " (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;southindia&#39;),\n",
       " (&#39;spark.hadoop.fs.idbfs.impl&#39;, &#39;com.databricks.io.idbfs.IdbfsFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n",
       "  &#39;08b3bfe1aba44374bd50d72383809c4b&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n",
       " (&#39;spark.hadoop.fs.gs.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.delta.sharing.profile.provider.class&#39;,\n",
       "  &#39;io.delta.sharing.DeltaSharingCredentialsProvider&#39;),\n",
       " (&#39;spark.databricks.managedCatalog.gcs.tokenProviderClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider&#39;),\n",
       " (&#39;spark.worker.aioaLazyConfig.iamReadinessCheckClientClass&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.NephosIamRoleCheckClient&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;autoscaling&#39;),\n",
       " (&#39;spark.databricks.automl.serviceEnabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n",
       "  &#39;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name&#39;),\n",
       " (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n",
       "  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.attribute_tag_service&#39;, &#39;&#39;),\n",
       " (&#39;spark.databricks.metrics.filesystem_io_metrics&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n",
       "  &#39;workerenv-8172574838468005&#39;),\n",
       " (&#39;spark.databricks.cloudfetch.requesterClassName&#39;,\n",
       "  &#39;com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester&#39;),\n",
       " (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;Standard_DS3_v2&#39;),\n",
       " (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.sparkContextId&#39;, &#39;4647372778270552063&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterSizeType&#39;, &#39;VM_CONTAINER&#39;),\n",
       " (&#39;spark.hadoop.databricks.fs.perfMetrics.enable&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.userProvidedSparkVersion&#39;,\n",
       "  &#39;10.4.x-scala2.12&#39;),\n",
       " (&#39;spark.hadoop.fs.gs.outputstream.upload.chunk.size&#39;, &#39;16777216&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;, &#39;20.235.50.151&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n",
       "  &#39;[{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;student_10f0d7tol6kadyw7_00826810@vocareumvocareum.onmicrosoft.com&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;student_10f0d7tol6kadyw7\\&#39;s Cluster&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;0830-030640-xkxwpaff&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8172574838468005&#34;}]&#39;),\n",
       " (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.privateLinkEnabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n",
       " (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.credential.aws.secretKey.redactor&#39;,\n",
       "  &#39;com.databricks.spark.util.AWSSecretKeyRedactorProxy&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;8172574838468005&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterNumCustomTags&#39;, &#39;0&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;,\n",
       "  &#39;287b82dab85846bb9e80b88392b3708d&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes&#39;,\n",
       "  &#39;false&#39;),\n",
       " (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n",
       " (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.workspace.multipleResults.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n",
       " (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType&#39;, &#39;default&#39;),\n",
       " (&#39;spark.databricks.acl.client&#39;,\n",
       "  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n",
       " (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.fs.mcfs-abfss.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.runtimeEngine&#39;, &#39;STANDARD&#39;),\n",
       " (&#39;spark.hadoop.fs.s3n.impl&#39;,\n",
       "  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n",
       "  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n",
       " (&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;),\n",
       " (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.fs.AbstractFileSystem.gs.impl&#39;,\n",
       "  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS&#39;),\n",
       " (&#39;spark.databricks.secret.sparkConf.keys.toRedact&#39;, &#39;&#39;),\n",
       " (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env&#39;, &#39;&#39;),\n",
       " (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n",
       " (&#39;spark.databricks.driverNfs.pathSuffix&#39;, &#39;.ephemeral_nfs&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n",
       " (&#39;spark.speculation&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n",
       " (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n",
       " (&#39;spark.master&#39;, &#39;spark://10.139.64.5:7077&#39;),\n",
       " (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n",
       "  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.max.total.tasks&#39;, &#39;1000&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;10&#39;),\n",
       " (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n",
       "  &#39;com.databricks.tahoe.store.MultiClusterLogStore&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterUnityCatalogMode&#39;,\n",
       "  &#39;LEGACY_SINGLE_USER_STANDARD&#39;),\n",
       " (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n",
       " (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n",
       " (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n",
       " (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n",
       " (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n",
       " (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n",
       " (&#39;spark.databricks.redactor&#39;,\n",
       "  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.impl&#39;,\n",
       "  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n",
       " (&#39;spark.databricks.workspaceUrl&#39;,\n",
       "  &#39;adb-8172574838468005.5.azuredatabricks.net&#39;),\n",
       " (&#39;spark.hadoop.fs.fcfs-abfs.impl.disable.cache&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n",
       " (&#39;spark.logConf&#39;, &#39;true&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n",
       " (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n",
       " (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n",
       " (&#39;spark.hadoop.parquet.filter.columnindex.enabled&#39;, &#39;false&#39;),\n",
       " (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n",
       " (&#39;spark.hadoop.fs.dbfsartifacts.impl&#39;,\n",
       "  &#39;com.databricks.backend.daemon.data.client.DBFSV1&#39;),\n",
       " (&#39;spark.hadoop.fs.cpfs-s3a.impl&#39;,\n",
       "  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n",
       " (&#39;spark.hadoop.fs.s3a.connection.timeout&#39;, &#39;50000&#39;),\n",
       " (&#39;spark.databricks.secret.envVar.keys.toRedact&#39;, &#39;&#39;),\n",
       " (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;Azure&#39;),\n",
       " (&#39;spark.files.useFetchCache&#39;, &#39;false&#39;)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[36]: [(&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;1&#39;),\n (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/databricks-hive/*&#39;),\n (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.sql.warehouse.dir&#39;, &#39;dbfs:/user/hive/warehouse&#39;),\n (&#39;spark.databricks.managedCatalog.clientClassName&#39;,\n  &#39;com.databricks.managedcatalog.ManagedCatalogClientImpl&#39;),\n (&#39;spark.app.id&#39;, &#39;app-20220902094120-0000&#39;),\n (&#39;spark.hadoop.fs.gs.impl&#39;,\n  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem&#39;),\n (&#39;spark.executor.extraJavaOptions&#39;,\n  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n (&#39;spark.executor.memory&#39;, &#39;7284m&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.retry.limit&#39;, &#39;20&#39;),\n (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLastActivityTime&#39;,\n  &#39;1661829498609&#39;),\n (&#39;spark.databricks.service.dbutils.repl.backend&#39;,\n  &#39;com.databricks.dbconnect.ReplDBUtils&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n (&#39;spark.databricks.acl.dfAclsEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.effectiveSparkVersion&#39;,\n  &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.fs.cpfs-adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException&#39;,\n  &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.driver.extraJavaOptions&#39;,\n  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED&#39;),\n (&#39;spark.databricks.clusterUsageTags.isIMv2Enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.hive.hmshandler.retry.interval&#39;, &#39;2000&#39;),\n (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.hadoop.fs.azure.authorization.caching.enable&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;Standard_DS3_v2&#39;),\n (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.userId&#39;, &#39;1708273445324017&#39;),\n (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterName&#39;,\n  &#34;student_10f0d7tol6kadyw7&#39;s Cluster&#34;),\n (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;, &#39;10.139.0.5&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;1&#39;),\n (&#39;spark.databricks.sql.configMapperClass&#39;,\n  &#39;com.databricks.dbsql.config.SqlConfigMapperBridge&#39;),\n (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.legacy.createHiveTableByDefault&#39;, &#39;false&#39;),\n (&#39;spark.ui.port&#39;, &#39;40001&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3a.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.attempts.maximum&#39;, &#39;10&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough&#39;,\n  &#39;false&#39;),\n (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;Standard_DS3_v2&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.retry.throttle.interval&#39;, &#39;500ms&#39;),\n (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n (&#39;spark.databricks.wsfsPublicPreview&#39;, &#39;true&#39;),\n (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes&#39;,\n  &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.hive.hmshandler.retry.attempts&#39;, &#39;10&#39;),\n (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmpQQVKWf&#39;),\n (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n (&#39;spark.sql.sources.default&#39;, &#39;delta&#39;),\n (&#39;spark.hadoop.fs.mcfs-gs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.hadoop.fs.cpfs-adl.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3n.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.cpfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.databricks.passthrough.oauth.refresher.impl&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient&#39;),\n (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n (&#39;spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMinWorkers&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.app.startTime&#39;, &#39;1662111677593&#39;),\n (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;1708273445324017&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n (&#39;spark.driver.host&#39;, &#39;10.139.64.5&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;southindia&#39;),\n (&#39;spark.hadoop.fs.elfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.credential.redactor&#39;,\n  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n (&#39;spark.databricks.acl.provider&#39;,\n  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n (&#39;spark.hadoop.parquet.abfs.readahead.optimization.enabled&#39;, &#39;false&#39;),\n (&#39;spark.extraListeners&#39;,\n  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;Standard_DS3_v2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMaxWorkers&#39;, &#39;2&#39;),\n (&#39;spark.hadoop.fs.dbfs.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DBFS&#39;),\n (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n (&#39;spark.hadoop.fs.cpfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.class&#39;,\n  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n (&#39;libraryDownload.sleepIntervalSeconds&#39;, &#39;5&#39;),\n (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n (&#39;spark.databricks.service.dbutils.server.backend&#39;,\n  &#39;com.databricks.dbconnect.SparkServerDBUtils&#39;),\n (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n (&#39;spark.databricks.managedCatalog.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider&#39;),\n (&#39;spark.databricks.repl.enableClassFileCleanup&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;, &#39;10.139.64.5&#39;),\n (&#39;spark.driver.port&#39;, &#39;36515&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled&#39;,\n  &#39;true&#39;),\n (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;[REDACTED]&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting&#39;,\n  &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape&#39;, &#39;false&#39;),\n (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n (&#39;spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass&#39;,\n  &#39;com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.managedResourceGroup&#39;,\n  &#39;databricks-rg-L4demo-oumcmsuqcy3hu&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n (&#39;libraryDownload.timeoutSeconds&#39;, &#39;180&#39;),\n (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.clusterUsageTags.azureSubscriptionId&#39;,\n  &#39;94ec3a64-dcfe-4219-9e29-88221690c382&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n  &#39;workerenv-8172574838468005&#39;),\n (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n (&#39;spark.hadoop.fs.s3a.block.size&#39;, &#39;67108864&#39;),\n (&#39;spark.databricks.tahoe.logStore.gcp.class&#39;,\n  &#39;com.databricks.tahoe.store.GCPLogStore&#39;),\n (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkMasterUrlType&#39;, &#39;None&#39;),\n (&#39;spark.databricks.passthrough.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.sources.commitProtocolClass&#39;,\n  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n (&#39;spark.hadoop.fs.abfss.impl&#39;,\n  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_budget&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n  &#39;azure_disk_volume_type: PREMIUM_LRS\\n&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0830-030640-xkxwpaff&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.file.impl&#39;,\n  &#39;com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer&#39;, &#39;&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.elfs.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND_AZURE&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_suite&#39;, &#39;&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasbs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;1&#39;),\n (&#39;spark.databricks.driverNfs.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n  &#39;RDS_DIRECT&#39;),\n (&#39;spark.databricks.clusterUsageTags.ngrokNpipEnabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.parquet.page.metadata.validation.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.acl.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.passthrough.glue.executorServiceFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory&#39;),\n (&#39;spark.repl.class.outputDir&#39;,\n  &#39;/local_disk0/tmp/repl/spark-4647372778270552063-8510db57-0394-4867-84a2-bc1aa57f65d6&#39;),\n (&#39;spark.hadoop.fs.abfs.impl&#39;,\n  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;true&#39;),\n (&#39;spark.databricks.acl.scim.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.DriverToWebappScimClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;true&#39;),\n (&#39;spark.executor.extraClassPath&#39;,\n  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----com_google_protobuf--timestamp_proto-spark_3.2_2.12-scalabp.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_2--common--kvstore--kvstore-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-common--network-common-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-shuffle--network-shuffle-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--sketch--sketch-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--tags--tags-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--unsafe--unsafe-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_2--core--proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--launcher--launcher-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.5.0-4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.gson--gson--com.google.code.gson__gson__2.8.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.istack--istack-commons-runtime--com.sun.istack__istack-commons-runtime__3.0.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill-java--com.twitter__chill-java__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill_2.12--com.twitter__chill_2.12__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.zaxxer--HikariCP--com.zaxxer__HikariCP__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-codec--commons-codec--commons-codec__commons-codec__1.15.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-io--commons-io--commons-io__commons-io__2.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--arpack--dev.ludovic.netlib__arpack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--blas--dev.ludovic.netlib__blas__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--lapack--dev.ludovic.netlib__lapack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.airlift--aircompressor--io.airlift__aircompressor__0.21.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.4.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.netty--netty-all--io.netty__netty-all__4.1.68.Final.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.servlet--jakarta.servlet-api--jakarta.servlet__jakarta.servlet-api__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.3.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--joda-time--joda-time--joda-time__joda-time__2.10.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.avro--avro--org.apache.avro__avro__1.10.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.10.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.avro--avro-mapred--org.apache.avro__avro-mapred__1.10.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.21.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.commons--commons-text--org.apache.commons__commons-text__1.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.curator--curator-client--org.apache.curator__curator-client__2.13.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.13.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.13.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.derby--derby--org.apache.derby__derby__10.14.2.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hadoop--hadoop-client-api--org.apache.hadoop__hadoop-client-api__3.3.1-databricks.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hadoop--hadoop-client-runtime--org.apache.hadoop__hadoop-client-runtime__3.3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.9.jar:/databricks/jars/----workspace_sp\n*** WARNING: skipped 82959 bytes of output ***\n\n (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.s3a.assumed.role.credentials.provider&#39;,\n  &#39;com.amazonaws.auth.InstanceProfileCredentialsProvider&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.active.blocks&#39;, &#39;32&#39;),\n (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.orgId&#39;, &#39;8172574838468005&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n (&#39;spark.repl.class.uri&#39;, &#39;spark://10.139.64.5:36515/classes&#39;),\n (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n (&#39;spark.databricks.cloudProvider&#39;, &#39;Azure&#39;),\n (&#39;spark.databricks.cloudfetch.hasRegionSupport&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories&#39;,\n  &#39;false&#39;),\n (&#39;spark.hadoop.fs.wasb.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.hadoop.fs.mcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.unityCatalog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.passthrough.glue.credentialsProviderFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory&#39;),\n (&#39;spark.sparklyr-backend.threads&#39;, &#39;1&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice&#39;, &#39;-1.0&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasb.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;southindia&#39;),\n (&#39;spark.hadoop.fs.idbfs.impl&#39;, &#39;com.databricks.io.idbfs.IdbfsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n  &#39;08b3bfe1aba44374bd50d72383809c4b&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n (&#39;spark.hadoop.fs.gs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.delta.sharing.profile.provider.class&#39;,\n  &#39;io.delta.sharing.DeltaSharingCredentialsProvider&#39;),\n (&#39;spark.databricks.managedCatalog.gcs.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider&#39;),\n (&#39;spark.worker.aioaLazyConfig.iamReadinessCheckClientClass&#39;,\n  &#39;com.databricks.backend.daemon.driver.NephosIamRoleCheckClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;autoscaling&#39;),\n (&#39;spark.databricks.automl.serviceEnabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n  &#39;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name&#39;),\n (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_service&#39;, &#39;&#39;),\n (&#39;spark.databricks.metrics.filesystem_io_metrics&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n  &#39;workerenv-8172574838468005&#39;),\n (&#39;spark.databricks.cloudfetch.requesterClassName&#39;,\n  &#39;com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester&#39;),\n (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;Standard_DS3_v2&#39;),\n (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n (&#39;spark.databricks.sparkContextId&#39;, &#39;4647372778270552063&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSizeType&#39;, &#39;VM_CONTAINER&#39;),\n (&#39;spark.hadoop.databricks.fs.perfMetrics.enable&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedSparkVersion&#39;,\n  &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.fs.gs.outputstream.upload.chunk.size&#39;, &#39;16777216&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;, &#39;20.235.50.151&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n  &#39;[{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;student_10f0d7tol6kadyw7_00826810@vocareumvocareum.onmicrosoft.com&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;student_10f0d7tol6kadyw7\\&#39;s Cluster&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;0830-030640-xkxwpaff&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8172574838468005&#34;}]&#39;),\n (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n (&#39;spark.databricks.clusterUsageTags.privateLinkEnabled&#39;, &#39;false&#39;),\n (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n (&#39;spark.databricks.credential.aws.secretKey.redactor&#39;,\n  &#39;com.databricks.spark.util.AWSSecretKeyRedactorProxy&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;8172574838468005&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumCustomTags&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;,\n  &#39;287b82dab85846bb9e80b88392b3708d&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes&#39;,\n  &#39;false&#39;),\n (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.multipleResults.enabled&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType&#39;, &#39;default&#39;),\n (&#39;spark.databricks.acl.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.mcfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.runtimeEngine&#39;, &#39;STANDARD&#39;),\n (&#39;spark.hadoop.fs.s3n.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n (&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;),\n (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.AbstractFileSystem.gs.impl&#39;,\n  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS&#39;),\n (&#39;spark.databricks.secret.sparkConf.keys.toRedact&#39;, &#39;&#39;),\n (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env&#39;, &#39;&#39;),\n (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n (&#39;spark.databricks.driverNfs.pathSuffix&#39;, &#39;.ephemeral_nfs&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n (&#39;spark.speculation&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n (&#39;spark.master&#39;, &#39;spark://10.139.64.5:7077&#39;),\n (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.s3a.max.total.tasks&#39;, &#39;1000&#39;),\n (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;10&#39;),\n (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n  &#39;com.databricks.tahoe.store.MultiClusterLogStore&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterUnityCatalogMode&#39;,\n  &#39;LEGACY_SINGLE_USER_STANDARD&#39;),\n (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n (&#39;spark.databricks.redactor&#39;,\n  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n (&#39;spark.hadoop.fs.s3a.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.workspaceUrl&#39;,\n  &#39;adb-8172574838468005.5.azuredatabricks.net&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.logConf&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n (&#39;spark.hadoop.parquet.filter.columnindex.enabled&#39;, &#39;false&#39;),\n (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n (&#39;spark.hadoop.fs.dbfsartifacts.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DBFSV1&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.timeout&#39;, &#39;50000&#39;),\n (&#39;spark.databricks.secret.envVar.keys.toRedact&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;Azure&#39;),\n (&#39;spark.files.useFetchCache&#39;, &#39;false&#39;)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7273c380-7d54-46c3-9104-0445fe48e74d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read the uploaded files into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "041bfef2-837e-4fc8-879d-e7a949d58de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "payments_df = spark.read.format(\"csv\") \\\n",
    "      .option(\"sep\", \",\") \\\n",
    "      .load(\"/FileStore/data/payments.csv\") \n",
    "\n",
    "trips_df = spark.read.format(\"csv\") \\\n",
    "      .option(\"sep\", \",\") \\\n",
    "      .load(\"/FileStore/data/trips.csv\") \n",
    "\n",
    "riders_df = spark.read.format(\"csv\") \\\n",
    "      .option(\"sep\", \",\") \\\n",
    "      .load(\"/FileStore/data/riders.csv\") \n",
    "\n",
    "stations_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"sep\",\",\")\\\n",
    "    .load(\"/FileStore/data/stations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ecf6344-434c-473d-acc5-f6a261d89aaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Renaming Columns and displaying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39de335a-fa76-460d-9f9e-d3970612ce27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
       "         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|rider_id|\n",
       "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
       "89E7AA6C29227EFF| classic_bike|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|\n",
       "0FEFDE2603568365| classic_bike|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|\n",
       "E6159D746B2DBB91|electric_bike|2021-02-09 19:10:18|2021-02-09 19:19:10|    KA1503000012|  TA1305000029|   70870|\n",
       "B32D3199F1C2E75B| classic_bike|2021-02-02 17:49:41|2021-02-02 17:54:06|             637|  TA1305000034|   58974|\n",
       "83E463F23575F4BF|electric_bike|2021-02-23 15:07:23|2021-02-23 15:22:37|           13216|  TA1309000055|   39608|\n",
       "BDAA7E3494E8D545|electric_bike|2021-02-24 15:43:33|2021-02-24 15:49:05|           18003|  KP1705001026|   36267|\n",
       "A772742351171257| classic_bike|2021-02-01 17:47:42|2021-02-01 17:48:33|    KP1705001026|  KP1705001026|   50104|\n",
       "295476889D9B79F8| classic_bike|2021-02-11 18:33:53|2021-02-11 18:35:09|           18003|         18003|   19618|\n",
       "362087194BA4CC9A| classic_bike|2021-02-27 15:13:39|2021-02-27 15:36:36|    KP1705001026|  KP1705001026|   16732|\n",
       "21630F715038CCB0| classic_bike|2021-02-20 08:59:42|2021-02-20 09:17:04|    KP1705001026|  KP1705001026|   57068|\n",
       "A977EB7FE7F5CD3A| classic_bike|2021-02-20 08:58:16|2021-02-20 08:58:41|    KP1705001026|  KP1705001026|   32712|\n",
       "8B868B03D6753C2A| classic_bike|2021-02-20 16:45:11|2021-02-20 16:59:47|    KP1705001026|  KP1705001026|   23227|\n",
       "BD331D658B9D2C31| classic_bike|2021-02-18 13:21:03|2021-02-18 13:25:20|             525|           520|   73221|\n",
       "8DFEA9BAFE6BAA62| classic_bike|2021-02-26 17:40:05|2021-02-26 17:42:49|           13253|  TA1309000050|   22163|\n",
       "27BE9F6E67AFD86C| classic_bike|2021-02-06 14:40:25|2021-02-06 14:55:50|             525|         15578|    7566|\n",
       "9B790D47A0A0F7F1| classic_bike|2021-02-19 23:25:40|2021-02-20 00:10:00|    KA1503000044|  KA1504000142|   71588|\n",
       "3C2DF72600B1DE6C| classic_bike|2021-02-18 23:20:10|2021-02-19 00:01:39|    KA1503000044|  KA1504000142|   38661|\n",
       "48A8D07ED9C7065C| classic_bike|2021-02-20 23:35:29|2021-02-21 00:17:18|    KA1503000044|  KA1504000142|   64751|\n",
       "BBFF2AAA0A3A1A26|electric_bike|2021-02-02 15:48:52|2021-02-02 16:03:40|    KA1504000140|         17660|   10721|\n",
       "030723CBA8CF05E7| classic_bike|2021-02-23 07:44:12|2021-02-23 07:48:57|    TA1305000032|         15542|   13281|\n",
       "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
       "only showing top 20 rows\n",
       "\n",
       "+----------+----------+------+--------+\n",
       "payment_id|      date|amount|rider_id|\n",
       "+----------+----------+------+--------+\n",
       "         1|2019-05-01|   9.0|    1000|\n",
       "         2|2019-06-01|   9.0|    1000|\n",
       "         3|2019-07-01|   9.0|    1000|\n",
       "         4|2019-08-01|   9.0|    1000|\n",
       "         5|2019-09-01|   9.0|    1000|\n",
       "         6|2019-10-01|   9.0|    1000|\n",
       "         7|2019-11-01|   9.0|    1000|\n",
       "         8|2019-12-01|   9.0|    1000|\n",
       "         9|2020-01-01|   9.0|    1000|\n",
       "        10|2020-02-01|   9.0|    1000|\n",
       "        11|2020-03-01|   9.0|    1000|\n",
       "        12|2020-04-01|   9.0|    1000|\n",
       "        13|2020-05-01|   9.0|    1000|\n",
       "        14|2020-06-01|   9.0|    1000|\n",
       "        15|2020-07-01|   9.0|    1000|\n",
       "        16|2020-08-01|   9.0|    1000|\n",
       "        17|2020-09-01|   9.0|    1000|\n",
       "        18|2020-10-01|   9.0|    1000|\n",
       "        19|2020-11-01|   9.0|    1000|\n",
       "        20|2020-12-01|   9.0|    1000|\n",
       "+----------+----------+------+--------+\n",
       "only showing top 20 rows\n",
       "\n",
       "+------------+--------------------+------------------+------------------+\n",
       "  station_id|                name|          latitude|         longitude|\n",
       "+------------+--------------------+------------------+------------------+\n",
       "         525|Glenwood Ave &amp; To...|         42.012701|-87.66605799999999|\n",
       "KA1503000012|  Clark St &amp; Lake St| 41.88579466666667|-87.63110066666668|\n",
       "         637|Wood St &amp; Chicago...|         41.895634|        -87.672069|\n",
       "       13216|  State St &amp; 33rd St|        41.8347335|       -87.6258275|\n",
       "       18003|Fairbanks St &amp; Su...| 41.89580766666667|-87.62025316666669|\n",
       "KP1705001026|LaSalle Dr &amp; Huro...|         41.894877|        -87.632326|\n",
       "       13253|Lincoln Ave &amp; Wav...|         41.948797|        -87.675278|\n",
       "KA1503000044|Rush St &amp; Hubbard St|         41.890173|-87.62618499999999|\n",
       "KA1504000140|Winchester Ave &amp; ...| 41.92403733333333|-87.67641483333334|\n",
       "TA1305000032|Clinton St &amp; Madi...|         41.882242|-87.64106600000001|\n",
       "TA1306000012| Wells St &amp; Huron St| 41.89475366666667|-87.63440200000001|\n",
       "       13133|Damen Ave &amp; Cortl...|41.915983000000004|        -87.677335|\n",
       "      SL-005|Indiana Ave &amp; Roo...|         41.867888|        -87.623041|\n",
       "       13235|Southport Ave &amp; W...|          41.94815|         -87.66394|\n",
       "TA1307000139| MLK Jr Dr &amp; 29th St|         41.842052|           -87.617|\n",
       "TA1305000009|Clark St &amp; Ida B ...|     41.8759326655|-87.63058453549999|\n",
       "       13276|Stockton Dr &amp; Wri...|        41.9313455|-87.63869133333333|\n",
       "TA1307000107|Sheridan Rd &amp; Mon...|          41.96167|         -87.65464|\n",
       "       13193|Larrabee St &amp; Web...|         41.921822|-87.64414000000001|\n",
       "KA1503000072|Wacker Dr &amp; Washi...|         41.883132|        -87.637321|\n",
       "+------------+--------------------+------------------+------------------+\n",
       "only showing top 20 rows\n",
       "\n",
       "+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n",
       "rider_id| first_name|last_name|             address|  birthday|account_start_date|account_end_date|is_member|\n",
       "+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n",
       "    1000|      Diana|    Clark| 1200 Alyssa Squares|1989-02-13|        2019-04-23|            null|     True|\n",
       "    1001|   Jennifer|    Smith|     397 Diana Ferry|1976-08-10|        2019-11-01|      2020-09-01|     True|\n",
       "    1002|      Karen|    Smith|644 Brittany Row ...|1998-08-10|        2022-02-04|            null|     True|\n",
       "    1003|      Bryan|  Roberts|996 Dickerson Tur...|1999-03-29|        2019-08-26|            null|    False|\n",
       "    1004|      Jesse|Middleton|7009 Nathan Expre...|1969-04-11|        2019-09-14|            null|     True|\n",
       "    1005|  Christine|Rodriguez|224 Washington Mi...|1974-08-27|        2020-03-24|            null|    False|\n",
       "    1006|     Alicia|   Taylor|   1137 Angela Locks|2004-01-30|        2020-11-27|      2021-12-01|     True|\n",
       "    1007|   Benjamin|Fernandez|   979 Phillips Ways|1988-01-11|        2016-12-11|            null|    False|\n",
       "    1008|       John| Crawford|    7691 Evans Court|1987-02-21|        2021-03-28|      2021-07-01|     True|\n",
       "    1009|   Victoria|   Ritter|9922 Jim Crest Ap...|1981-02-07|        2020-06-12|      2021-11-01|     True|\n",
       "    1010|      Tracy|   Austin|    92973 Mary Ville|1996-04-07|        2019-12-27|            null|     True|\n",
       "    1011|    Jessica|    Mcgee|950 Grimes Burg A...|1984-12-29|        2017-05-20|            null|     True|\n",
       "    1012|    Heather|   Fisher|65532 Davis Sprin...|1980-10-20|        2021-10-16|            null|     True|\n",
       "    1013|    Timothy|    Jones| 7757 Johnston Roads|1985-07-10|        2020-12-28|      2021-11-01|     True|\n",
       "    1014|   Jennifer|   Martin|   501 Arellano Land|1989-12-04|        2017-11-24|            null|     True|\n",
       "    1015|Christopher|    Silva|3710 Rodriguez Gl...|2001-07-25|        2017-07-10|            null|     True|\n",
       "    1016|     Andrew|    Jones|  72226 Casey Square|1991-12-13|        2022-02-02|            null|     True|\n",
       "    1017|    William|   Lawson| 40395 Terrell Parks|1981-04-17|        2019-03-11|            null|     True|\n",
       "    1018|     Shelly|   Briggs|   3514 Leslie Vista|1986-09-02|        2021-07-25|            null|    False|\n",
       "    1019|       Tina|   Garcia|00348 Brandi Park...|1997-05-03|        2021-07-10|      2022-01-01|    False|\n",
       "+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|rider_id|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n|89E7AA6C29227EFF| classic_bike|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|\n|0FEFDE2603568365| classic_bike|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|\n|E6159D746B2DBB91|electric_bike|2021-02-09 19:10:18|2021-02-09 19:19:10|    KA1503000012|  TA1305000029|   70870|\n|B32D3199F1C2E75B| classic_bike|2021-02-02 17:49:41|2021-02-02 17:54:06|             637|  TA1305000034|   58974|\n|83E463F23575F4BF|electric_bike|2021-02-23 15:07:23|2021-02-23 15:22:37|           13216|  TA1309000055|   39608|\n|BDAA7E3494E8D545|electric_bike|2021-02-24 15:43:33|2021-02-24 15:49:05|           18003|  KP1705001026|   36267|\n|A772742351171257| classic_bike|2021-02-01 17:47:42|2021-02-01 17:48:33|    KP1705001026|  KP1705001026|   50104|\n|295476889D9B79F8| classic_bike|2021-02-11 18:33:53|2021-02-11 18:35:09|           18003|         18003|   19618|\n|362087194BA4CC9A| classic_bike|2021-02-27 15:13:39|2021-02-27 15:36:36|    KP1705001026|  KP1705001026|   16732|\n|21630F715038CCB0| classic_bike|2021-02-20 08:59:42|2021-02-20 09:17:04|    KP1705001026|  KP1705001026|   57068|\n|A977EB7FE7F5CD3A| classic_bike|2021-02-20 08:58:16|2021-02-20 08:58:41|    KP1705001026|  KP1705001026|   32712|\n|8B868B03D6753C2A| classic_bike|2021-02-20 16:45:11|2021-02-20 16:59:47|    KP1705001026|  KP1705001026|   23227|\n|BD331D658B9D2C31| classic_bike|2021-02-18 13:21:03|2021-02-18 13:25:20|             525|           520|   73221|\n|8DFEA9BAFE6BAA62| classic_bike|2021-02-26 17:40:05|2021-02-26 17:42:49|           13253|  TA1309000050|   22163|\n|27BE9F6E67AFD86C| classic_bike|2021-02-06 14:40:25|2021-02-06 14:55:50|             525|         15578|    7566|\n|9B790D47A0A0F7F1| classic_bike|2021-02-19 23:25:40|2021-02-20 00:10:00|    KA1503000044|  KA1504000142|   71588|\n|3C2DF72600B1DE6C| classic_bike|2021-02-18 23:20:10|2021-02-19 00:01:39|    KA1503000044|  KA1504000142|   38661|\n|48A8D07ED9C7065C| classic_bike|2021-02-20 23:35:29|2021-02-21 00:17:18|    KA1503000044|  KA1504000142|   64751|\n|BBFF2AAA0A3A1A26|electric_bike|2021-02-02 15:48:52|2021-02-02 16:03:40|    KA1504000140|         17660|   10721|\n|030723CBA8CF05E7| classic_bike|2021-02-23 07:44:12|2021-02-23 07:48:57|    TA1305000032|         15542|   13281|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\nonly showing top 20 rows\n\n+----------+----------+------+--------+\n|payment_id|      date|amount|rider_id|\n+----------+----------+------+--------+\n|         1|2019-05-01|   9.0|    1000|\n|         2|2019-06-01|   9.0|    1000|\n|         3|2019-07-01|   9.0|    1000|\n|         4|2019-08-01|   9.0|    1000|\n|         5|2019-09-01|   9.0|    1000|\n|         6|2019-10-01|   9.0|    1000|\n|         7|2019-11-01|   9.0|    1000|\n|         8|2019-12-01|   9.0|    1000|\n|         9|2020-01-01|   9.0|    1000|\n|        10|2020-02-01|   9.0|    1000|\n|        11|2020-03-01|   9.0|    1000|\n|        12|2020-04-01|   9.0|    1000|\n|        13|2020-05-01|   9.0|    1000|\n|        14|2020-06-01|   9.0|    1000|\n|        15|2020-07-01|   9.0|    1000|\n|        16|2020-08-01|   9.0|    1000|\n|        17|2020-09-01|   9.0|    1000|\n|        18|2020-10-01|   9.0|    1000|\n|        19|2020-11-01|   9.0|    1000|\n|        20|2020-12-01|   9.0|    1000|\n+----------+----------+------+--------+\nonly showing top 20 rows\n\n+------------+--------------------+------------------+------------------+\n|  station_id|                name|          latitude|         longitude|\n+------------+--------------------+------------------+------------------+\n|         525|Glenwood Ave &amp; To...|         42.012701|-87.66605799999999|\n|KA1503000012|  Clark St &amp; Lake St| 41.88579466666667|-87.63110066666668|\n|         637|Wood St &amp; Chicago...|         41.895634|        -87.672069|\n|       13216|  State St &amp; 33rd St|        41.8347335|       -87.6258275|\n|       18003|Fairbanks St &amp; Su...| 41.89580766666667|-87.62025316666669|\n|KP1705001026|LaSalle Dr &amp; Huro...|         41.894877|        -87.632326|\n|       13253|Lincoln Ave &amp; Wav...|         41.948797|        -87.675278|\n|KA1503000044|Rush St &amp; Hubbard St|         41.890173|-87.62618499999999|\n|KA1504000140|Winchester Ave &amp; ...| 41.92403733333333|-87.67641483333334|\n|TA1305000032|Clinton St &amp; Madi...|         41.882242|-87.64106600000001|\n|TA1306000012| Wells St &amp; Huron St| 41.89475366666667|-87.63440200000001|\n|       13133|Damen Ave &amp; Cortl...|41.915983000000004|        -87.677335|\n|      SL-005|Indiana Ave &amp; Roo...|         41.867888|        -87.623041|\n|       13235|Southport Ave &amp; W...|          41.94815|         -87.66394|\n|TA1307000139| MLK Jr Dr &amp; 29th St|         41.842052|           -87.617|\n|TA1305000009|Clark St &amp; Ida B ...|     41.8759326655|-87.63058453549999|\n|       13276|Stockton Dr &amp; Wri...|        41.9313455|-87.63869133333333|\n|TA1307000107|Sheridan Rd &amp; Mon...|          41.96167|         -87.65464|\n|       13193|Larrabee St &amp; Web...|         41.921822|-87.64414000000001|\n|KA1503000072|Wacker Dr &amp; Washi...|         41.883132|        -87.637321|\n+------------+--------------------+------------------+------------------+\nonly showing top 20 rows\n\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|rider_id| first_name|last_name|             address|  birthday|account_start_date|account_end_date|is_member|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\n|    1000|      Diana|    Clark| 1200 Alyssa Squares|1989-02-13|        2019-04-23|            null|     True|\n|    1001|   Jennifer|    Smith|     397 Diana Ferry|1976-08-10|        2019-11-01|      2020-09-01|     True|\n|    1002|      Karen|    Smith|644 Brittany Row ...|1998-08-10|        2022-02-04|            null|     True|\n|    1003|      Bryan|  Roberts|996 Dickerson Tur...|1999-03-29|        2019-08-26|            null|    False|\n|    1004|      Jesse|Middleton|7009 Nathan Expre...|1969-04-11|        2019-09-14|            null|     True|\n|    1005|  Christine|Rodriguez|224 Washington Mi...|1974-08-27|        2020-03-24|            null|    False|\n|    1006|     Alicia|   Taylor|   1137 Angela Locks|2004-01-30|        2020-11-27|      2021-12-01|     True|\n|    1007|   Benjamin|Fernandez|   979 Phillips Ways|1988-01-11|        2016-12-11|            null|    False|\n|    1008|       John| Crawford|    7691 Evans Court|1987-02-21|        2021-03-28|      2021-07-01|     True|\n|    1009|   Victoria|   Ritter|9922 Jim Crest Ap...|1981-02-07|        2020-06-12|      2021-11-01|     True|\n|    1010|      Tracy|   Austin|    92973 Mary Ville|1996-04-07|        2019-12-27|            null|     True|\n|    1011|    Jessica|    Mcgee|950 Grimes Burg A...|1984-12-29|        2017-05-20|            null|     True|\n|    1012|    Heather|   Fisher|65532 Davis Sprin...|1980-10-20|        2021-10-16|            null|     True|\n|    1013|    Timothy|    Jones| 7757 Johnston Roads|1985-07-10|        2020-12-28|      2021-11-01|     True|\n|    1014|   Jennifer|   Martin|   501 Arellano Land|1989-12-04|        2017-11-24|            null|     True|\n|    1015|Christopher|    Silva|3710 Rodriguez Gl...|2001-07-25|        2017-07-10|            null|     True|\n|    1016|     Andrew|    Jones|  72226 Casey Square|1991-12-13|        2022-02-02|            null|     True|\n|    1017|    William|   Lawson| 40395 Terrell Parks|1981-04-17|        2019-03-11|            null|     True|\n|    1018|     Shelly|   Briggs|   3514 Leslie Vista|1986-09-02|        2021-07-25|            null|    False|\n|    1019|       Tina|   Garcia|00348 Brandi Park...|1997-05-03|        2021-07-10|      2022-01-01|    False|\n+--------+-----------+---------+--------------------+----------+------------------+----------------+---------+\nonly showing top 20 rows\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trips_list = [\"trip_id\" ,\"rideable_type\", \"start_at\", \"ended_at\", \"start_station_id\", \"end_station_id\", \"rider_id\"] \n",
    "trips_df = trips_df.toDF(*trips_list)\n",
    "trips_df.show()\n",
    "\n",
    "payments_list = [\"payment_id\",\"date\",\"amount\",\"rider_id\" ]\n",
    "payments_df = payments_df.toDF(*payments_list)\n",
    "payments_df.show()\n",
    "\n",
    "stations_list = [\"station_id\",\"name\",\"latitude\",\"longitude\" ]\n",
    "stations_df = stations_df.toDF(*stations_list)\n",
    "stations_df.show()\n",
    "\n",
    "riders_list = [\"rider_id\", \"first_name\", \"last_name\", \"address\", \"birthday\", \"account_start_date\", \"account_end_date\", \"is_member\" ]\n",
    "riders_df = riders_df.toDF(*riders_list)\n",
    "riders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09f416b3-dd3f-4bab-a9cb-ef8fc266c210",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creation of Bronze Layer. These tables hold the data in the raw format (same as the files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "675e012c-2969-4ee8-9f7b-444116ade419",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Store into delta files\n",
    "payments_df.write.format(\"delta\").mode(\"overwrite\").save(\"payments_data\")\n",
    "trips_df.write.format(\"delta\").mode(\"overwrite\").save(\"trips_data\")\n",
    "riders_df.write.format(\"delta\").mode(\"overwrite\").save(\"riders_data\")\n",
    "stations_df.write.format(\"delta\").mode(\"overwrite\").save(\"stations_data\")\n",
    "\n",
    "#create bronze layer tables\n",
    "spark.sql(\"CREATE TABLE bronze_payments USING DELTA LOCATION '/delta/payments_data'\")\n",
    "spark.sql(\"CREATE TABLE bronze_trips USING DELTA LOCATION '/delta/trips_data'\")\n",
    "spark.sql(\"CREATE TABLE bronze_riders USING DELTA LOCATION '/delta/riders_data'\")\n",
    "spark.sql(\"CREATE TABLE bronze_stations USING DELTA LOCATION '/delta/stations_data'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84dc12a5-ce20-496e-83bb-338938f6492e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creation of Silver Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35d7b530-5303-44e0-bb8f-cfd22e0d41d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating the Date Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00467596-04cc-4347-98be-ec22c48ae1e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dates = spark.sql(\"SELECT to_timestamp(max(ended_at)),to_timestamp(date_add(min(start_at), -1500)) FROM bronze_trips\")\n",
    "beginDate = dates.first()[1]\n",
    "endDate = dates.first()[0]\n",
    "\n",
    "(\n",
    "  spark.sql(f\"select explode(sequence(to_timestamp('{beginDate}'), to_timestamp('{endDate}'), interval 1 day)) as calendarDate\")\n",
    "    .createOrReplaceTempView('dimdates')\n",
    ")\n",
    "\n",
    "dimDate_df = spark.sql('''SELECT calendarDate as date_key,\n",
    "                    calendarDate as datetime,\n",
    "                    date_part('YEAR',calendarDate)  as Year,\n",
    "                    date_part('QUARTER', calendarDate) as Quarter,\n",
    "                    date_part('MONTH', calendarDate) as Month,\n",
    "                    date_part('DAY',calendarDate) as Day,\n",
    "                    date_part('WEEK',calendarDate) as Week,\n",
    "                    date_part('HOUR',calendarDate) as Hour,\n",
    "                    date_part('MINUTE',calendarDate) as Minute,\n",
    "                    date_part('SECOND',calendarDate) as Second,\n",
    "                    CASE WHEN date_part('DAYOFWEEK',calendarDate) IN (6,7) THEN 1 ELSE 0 END as is_weekend                    \n",
    "                    FROM dimDates\n",
    "                    ''')\n",
    "dimDate_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_dimDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28fdf0ed-1b70-45db-837b-3b2672113926",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating the Rider Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7be1b758-bb2a-4c22-8005-0e2e3b0e75a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dimRider_df = spark.sql('''SELECT DISTINCT rider_id as rider_key,\n",
    "                            rider_id as rider_id,\n",
    "                            first_name as first_name,\n",
    "                            last_name as last_name,\n",
    "                            to_timestamp(birthday) as birthday,\n",
    "                            is_member as ismember\n",
    "                            FROM bronze_riders\n",
    "                            ''')\n",
    "dimRider_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_dimRider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ea44b23-9a1d-4b6e-82c4-15f3e35b26e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating the Station Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b92859b8-b179-44e6-b0de-b996d20511b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dimStation_df = spark.sql('''SELECT DISTINCT station_id as station_key,\n",
    "                            station_id as station_id,\n",
    "                            name as name,\n",
    "                            latitude as latitude,\n",
    "                            longitude as longitude\n",
    "                            FROM bronze_stations''');\n",
    "\n",
    "dimStation_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_dimStation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "968239fe-2551-4152-9199-f00e595beee4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating the Payments Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "704f4868-066b-46d2-8825-5f2d60954669",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "factPayment_df = spark.sql('''SELECT DISTINCT p.payment_id as payment_key,\n",
    "                                p.payment_id as payment_id,\n",
    "                                p.amount as amount,\n",
    "                                d.date_key as date_key,\n",
    "                                r.rider_key as rider_key\n",
    "                                FROM bronze_payments p\n",
    "                                JOIN silver_dimDate d ON p.date = to_date(d.date_key)\n",
    "                                JOIN silver_dimRider r on p.rider_id = r.rider_key''')\n",
    "factPayment_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_factPayment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff3adb36-ae56-4692-ad7e-9bb48114de27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating the Trips Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9b0025a-2f24-40e9-9ee2-9e7c0910d550",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "factTrip_df = spark.sql('''SELECT  DISTINCT trip.trip_id as trip_key,\n",
    "                    trip.trip_id as trip_id,\n",
    "                    td1.date_key as start_date_key,  \n",
    "                    td2.date_key as end_date_key,               \n",
    "                    rider.rider_key as rider_key,\n",
    "                    DATEDIFF(second, trip.start_at,trip.ended_at) as duration,\n",
    "                    st1.station_key as start_station_key,\n",
    "                    st2.station_key as end_station_key,\n",
    "                    DATEDIFF(MONTH, rider.birthday,trip.start_at) as rider_age,                    \n",
    "                    trip.rideable_type as rideable_type\n",
    "            FROM bronze_trips trip\n",
    "            LEFT JOIN silver_dimDate td1 ON date_trunc(\"Day\",td1.datetime) = date_trunc(\"Day\",trip.start_at)\n",
    "            LEFT JOIN silver_dimDate td2 ON DATE_TRUNC(\"Day\",td2.datetime) = DATE_TRUNC(\"Day\",trip.ended_at)\n",
    "            JOIN silver_dimRider rider ON trip.rider_id = rider.rider_id\n",
    "            JOIN silver_dimStation st1 ON trip.start_station_id = st1.station_id\n",
    "            JOIN silver_dimStation st2 ON trip.end_station_id = st2.station_id''')\n",
    "factTrip_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_factTrip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43297ff5-b419-4eea-962d-e37ef14df652",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Business Queries for which Gold Layer tables are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e191f03a-c029-485c-ab45-4a445b22ffdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyze how much time is spent per ride Based on time of day\n",
    "### gold_duration_by_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71abb6ca-c263-417c-b92b-d77ef6e9b171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dur_by_hour_df = spark.sql('''SELECT AVG(DURATION) , dt.HOUR\n",
    "FROM silver_factTrip trip\n",
    "INNER JOIN silver_dimDate dt\n",
    "ON trip.start_date_key = dt.start_date_key\n",
    "group by dt.hour''')\n",
    "dur_by_hour_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_duration_by_hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b376d7f-5fbb-4909-81c6-ba4d49cb70f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyze how much time is spent per ride Based on which station is the starting and / or ending station\n",
    "### gold_duration_by_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dde1d44-222b-42f3-9f40-cb577c4bd11f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dur_by_station_df = spark.sql('''SELECT AVG(trip.duration),st.name\n",
    "FROM silver_factTrip trip\n",
    "INNER JOIN silver_dimStation st\n",
    "ON trip.start_station_key = st.station_key\n",
    "WHERE ST.NAME = 'Glenwood Ave & Touhy Ave'\n",
    "GROUP BY st.name''')\n",
    "dur_by_station_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_duration_by_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f3152c4-99f9-44e9-af39-397bd5d98796",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyze how much time is spent per ride Based on age of the rider at time of the ride\n",
    "### gold_duration_by_riderage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d86aca1a-e575-484f-9787-69e2690680aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dur_by_age_df = spark.sql('''\n",
    "SELECT AVG(duration), age\n",
    "FROM silver_factTrip\n",
    "group by age''')\n",
    "dur_by_age_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_duration_by_riderage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cc196d09-61de-4da1-a8b7-7bf67c626d4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyze how much money is spent month\n",
    "### gold_amount_by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55031dff-56f2-471a-82a8-ed2050ea3f93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "amt_by_mnt_df = spark.sql('''SELECT SUM(amount) , dt.month, dt.year, dt.quarter\n",
    "FROM silver_factPayment p\n",
    "INNER JOIN silver dimDate dt\n",
    "ON p.date_key = dt.date_key\n",
    "where dt.year = '2021'\n",
    "GROUP BY dt.month, dt.year, dt.quarter''')\n",
    "amt_by_mnt_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_amount_by_month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab1dea4d-dd9e-4849-9d54-092a4e1ab603",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyze how much money is spent Per member, based on the age of the rider at account start\n",
    "### gold_amount_by_rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e512a31a-63df-4894-b04d-8f3b08e09312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "amount_by_rider_df = spark.sql('''SELECT SUM(AMOUNT), R.FIRST_NAME\n",
    "FROM silver_factPayment p\n",
    "INNER JOIN silver_dimRider r\n",
    "WHERE r.age = 25\n",
    "GROUP BY r.first_name''')\n",
    "amount_by_rider_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_amount_by_rider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb96291b-6395-4f58-8c5f-326c6abbfed4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyze how much money is spent per member Based on how many rides the rider averages per month\n",
    "### gold_rides_per_rider_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a50eb195-4a4a-4120-9db6-9c96a301dfa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rides_df = spark.sql('''SELECT count(trip_id), r.first_name,r.rider_key, dt.month\n",
    "FROM silver_factTrip t\n",
    "INNER JOIN silver_dimRider r ON t.rider_key = r.rider_key\n",
    "INNER JOIN silver_dimDate d ON t.date_key = d.date_key\n",
    "GROUP BY r.first_name , r.rider_key,dt.month''')\n",
    "rides_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_rides_per_rider_month\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23a69411-56d2-4f69-b5eb-aba8c1327187",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analyze how much money is spent per member Based on how many minutes the rider spends on a bike per month\n",
    "### gold_duration_rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "175ca2ac-a163-4356-aa99-d71d34398378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dur_rider_df = spark.sql('''SELECT sum(minutes) , r.rider_key, r.first_name\n",
    "FROM silver_factTrip t\n",
    "INNER JOIN silver_dimDate d ON t.date_key = d.date_key\n",
    "INNER JOIN silver_dimRider r on t.rider_key = r.rider_key''')\n",
    "dur_rider_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_duration_rider\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project4",
   "notebookOrigID": 2609142479093236,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
